#!/usr/bin/env python3
"""
Comprehensive Nancy Validation Suite

Master test orchestrator that runs the complete validation of Nancy's 
four-brain architecture against the enhanced baseline RAG system.

This represents the culmination of our major enhancements and provides
definitive evidence of Nancy's value proposition for engineering teams.

Test Coverage:
1. Enhanced baseline capabilities validation
2. Nancy's four-brain spreadsheet intelligence  
3. Codebase analysis with AST and Git integration
4. Cross-data-type relationship discovery
5. Directory change intelligence (unique to Nancy)
6. Engineering team value scenarios
7. Performance and cost analysis

Generates executive summary with clear ROI analysis and deployment recommendations.
"""

import json
import time
import os
import subprocess
import sys
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path

# Import our test modules
try:
    from comprehensive_enhanced_benchmark import EnhancedComprehensiveBenchmark
    from test_codebase_intelligence import CodebaseIntelligenceTest  
    from test_change_intelligence import ChangeIntelligenceTest
    from test_baseline_spreadsheet_queries import test_spreadsheet_specific_queries
except ImportError as e:
    print(f"Error importing test modules: {e}")
    print("Please ensure all test modules are in the current directory")
    sys.exit(1)

class ComprehensiveValidationSuite:
    def __init__(self):
        self.nancy_url = "http://localhost:8000"
        self.baseline_url = "http://localhost:8002"
        self.test_start_time = datetime.now()
        self.results = {}
        self.executive_summary = {}
        
        # Test execution tracking
        self.test_phases = [
            {
                "name": "System Health Check",
                "description": "Verify Nancy and Baseline systems are operational",
                "critical": True
            },
            {
                "name": "Enhanced Baseline Validation", 
                "description": "Confirm baseline can handle spreadsheet queries (fair comparison)",
                "critical": True
            },
            {
                "name": "Comprehensive Capability Benchmark",
                "description": "Compare Nancy vs Baseline across all data types and scenarios",
                "critical": True
            },
            {
                "name": "Codebase Intelligence Testing",
                "description": "Validate AST parsing, Git integration, and code understanding",
                "critical": False
            },
            {
                "name": "Change Intelligence Testing",
                "description": "Test unique change detection and impact analysis (Nancy only)",
                "critical": False
            },
            {
                "name": "Executive Analysis",
                "description": "Generate ROI analysis and deployment recommendations",
                "critical": True
            }
        ]
    
    def check_system_health(self) -> Dict[str, Any]:
        """Comprehensive system health check"""
        print("üè• SYSTEM HEALTH CHECK")
        print("=" * 50)
        
        health_results = {
            "nancy": {"status": "unknown", "details": {}},
            "baseline": {"status": "unknown", "details": {}},
            "overall_health": "unknown"
        }
        
        # Test Nancy
        print("   Testing Nancy Four-Brain System...")
        try:
            import requests
            response = requests.get(f"{self.nancy_url}/health", timeout=30)
            if response.status_code == 200:
                health_data = response.json()
                health_results["nancy"] = {
                    "status": "healthy",
                    "details": health_data,
                    "brains_operational": self._count_operational_brains(health_data)
                }
                print(f"     ‚úì Nancy is healthy ({health_results['nancy']['brains_operational']}/4 brains operational)")
            else:
                health_results["nancy"] = {
                    "status": "unhealthy", 
                    "error": f"HTTP {response.status_code}",
                    "details": response.text
                }
                print(f"     ‚úó Nancy unhealthy: HTTP {response.status_code}")
        except Exception as e:
            health_results["nancy"] = {"status": "error", "error": str(e)}
            print(f"     ‚úó Nancy error: {e}")
        
        # Test Enhanced Baseline
        print("   Testing Enhanced Baseline RAG System...")
        try:
            response = requests.get(f"{self.baseline_url}/health", timeout=30)
            if response.status_code == 200:
                health_data = response.json()
                health_results["baseline"] = {
                    "status": "healthy",
                    "details": health_data
                }
                print("     ‚úì Enhanced Baseline is healthy")
            else:
                health_results["baseline"] = {
                    "status": "unhealthy",
                    "error": f"HTTP {response.status_code}",
                    "details": response.text  
                }
                print(f"     ‚úó Baseline unhealthy: HTTP {response.status_code}")
        except Exception as e:
            health_results["baseline"] = {"status": "error", "error": str(e)}
            print(f"     ‚úó Baseline error: {e}")
        
        # Overall health assessment
        nancy_healthy = health_results["nancy"]["status"] == "healthy"
        baseline_healthy = health_results["baseline"]["status"] == "healthy"
        
        if nancy_healthy and baseline_healthy:
            health_results["overall_health"] = "excellent"
            print("\n   üéâ Both systems are healthy - proceeding with full validation")
        elif nancy_healthy or baseline_healthy:
            health_results["overall_health"] = "partial"
            print("\n   ‚ö†Ô∏è  One system has issues - validation may be limited")
        else:
            health_results["overall_health"] = "poor"
            print("\n   ‚ùå Both systems have issues - validation cannot proceed")
        
        return health_results
    
    def _count_operational_brains(self, health_data: Dict[str, Any]) -> int:
        """Count operational brains from Nancy health data"""
        brain_indicators = ["vector_brain", "analytical_brain", "graph_brain", "linguistic_brain"]
        operational = 0
        
        for brain in brain_indicators:
            if brain in health_data and health_data[brain].get("status") == "healthy":
                operational += 1
        
        return operational
    
    def run_enhanced_baseline_validation(self) -> Dict[str, Any]:
        """Validate that enhanced baseline provides fair comparison"""
        print("\nüìä ENHANCED BASELINE VALIDATION")
        print("=" * 50)
        print("Confirming baseline can handle spreadsheet queries for fair comparison...")
        
        try:
            # Run baseline spreadsheet query tests
            fairness_achieved = test_spreadsheet_specific_queries()
            
            baseline_validation = {
                "fairness_achieved": fairness_achieved,
                "comparison_validity": "fair" if fairness_achieved else "unfair",
                "baseline_enhancement_successful": fairness_achieved,
                "notes": "Enhanced baseline with textification vs Nancy's structured intelligence" if fairness_achieved else "Baseline may need further enhancement"
            }
            
            if fairness_achieved:
                print("   ‚úì Enhanced baseline successfully handles spreadsheet queries")
                print("   ‚úì Fair comparison established - Nancy vs Enhanced Baseline")
            else:
                print("   ‚ö†Ô∏è  Baseline struggles with spreadsheet queries")
                print("   ‚ö†Ô∏è  Comparison may favor Nancy unfairly")
            
            return baseline_validation
            
        except Exception as e:
            return {
                "fairness_achieved": False,
                "error": str(e),
                "comparison_validity": "unknown"
            }
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """Run comprehensive capability benchmark"""
        print("\nüöÄ COMPREHENSIVE CAPABILITY BENCHMARK")
        print("=" * 50)
        
        try:
            benchmark = EnhancedComprehensiveBenchmark()
            results = benchmark.run_comprehensive_enhanced_benchmark()
            
            if results:
                print("   ‚úì Comprehensive benchmark completed successfully")
                return results
            else:
                print("   ‚úó Comprehensive benchmark failed")
                return {"status": "failed", "error": "Benchmark returned no results"}
                
        except Exception as e:
            print(f"   ‚úó Comprehensive benchmark error: {e}")
            return {"status": "error", "error": str(e)}
    
    def run_codebase_intelligence_test(self) -> Dict[str, Any]:
        """Run codebase intelligence testing"""
        print("\nüß† CODEBASE INTELLIGENCE TESTING")
        print("=" * 50)
        
        try:
            tester = CodebaseIntelligenceTest()
            results = tester.run_codebase_intelligence_tests()
            
            if results:
                print("   ‚úì Codebase intelligence testing completed")
                return results
            else:
                print("   ‚úó Codebase intelligence testing failed")
                return {"status": "failed", "error": "Test returned no results"}
                
        except Exception as e:
            print(f"   ‚úó Codebase intelligence test error: {e}")
            return {"status": "error", "error": str(e)}
    
    def run_change_intelligence_test(self) -> Dict[str, Any]:
        """Run change intelligence testing"""  
        print("\nüîÑ CHANGE INTELLIGENCE TESTING")
        print("=" * 50)
        
        try:
            tester = ChangeIntelligenceTest()
            results = tester.run_change_intelligence_tests()
            
            if results:
                print("   ‚úì Change intelligence testing completed")
                return results
            else:
                print("   ‚úó Change intelligence testing failed")
                return {"status": "failed", "error": "Test returned no results"}
                
        except Exception as e:
            print(f"   ‚úó Change intelligence test error: {e}")
            return {"status": "error", "error": str(e)}
    
    def generate_executive_analysis(self) -> Dict[str, Any]:
        """Generate executive analysis and recommendations"""
        print("\nüìà EXECUTIVE ANALYSIS")
        print("=" * 50)
        
        # Collect key metrics from all tests
        executive_metrics = {
            "system_reliability": self._assess_system_reliability(),
            "capability_comparison": self._assess_capability_comparison(),
            "unique_value_factors": self._identify_unique_value_factors(),
            "cost_benefit_analysis": self._perform_cost_benefit_analysis(),
            "engineering_team_roi": self._calculate_engineering_roi(),
            "deployment_readiness": self._assess_deployment_readiness(),
            "competitive_advantage": self._assess_competitive_advantage()
        }
        
        # Generate final recommendation
        final_recommendation = self._generate_final_recommendation(executive_metrics)
        
        executive_analysis = {
            "executive_metrics": executive_metrics,
            "final_recommendation": final_recommendation,
            "key_decision_factors": self._extract_key_decision_factors(executive_metrics),
            "implementation_timeline": self._suggest_implementation_timeline(final_recommendation),
            "risk_assessment": self._perform_risk_assessment(executive_metrics)
        }
        
        self._print_executive_summary(executive_analysis)
        
        return executive_analysis
    
    def _assess_system_reliability(self) -> Dict[str, Any]:
        """Assess system reliability from health checks"""
        health_results = self.results.get("system_health", {})
        
        nancy_healthy = health_results.get("nancy", {}).get("status") == "healthy"
        baseline_healthy = health_results.get("baseline", {}).get("status") == "healthy"
        brains_operational = health_results.get("nancy", {}).get("brains_operational", 0)
        
        return {
            "nancy_reliability": "high" if nancy_healthy and brains_operational >= 3 else "moderate" if nancy_healthy else "low",
            "baseline_reliability": "high" if baseline_healthy else "low", 
            "four_brain_architecture": f"{brains_operational}/4 brains operational",
            "overall_system_stability": "stable" if nancy_healthy and baseline_healthy else "unstable"
        }
    
    def _assess_capability_comparison(self) -> Dict[str, Any]:
        """Assess capability comparison from benchmark results"""
        benchmark_results = self.results.get("comprehensive_benchmark", {})
        
        if not benchmark_results or "comprehensive_analysis" not in benchmark_results:
            return {"status": "insufficient_data", "analysis": "Benchmark data unavailable"}
        
        analysis = benchmark_results["comprehensive_analysis"]
        overall_perf = analysis.get("overall_performance", {})
        
        nancy_advantage_rate = overall_perf.get("nancy_advantage_rate", 0)
        intelligence_premium = analysis.get("intelligence_cost_analysis", {}).get("intelligence_time_premium", 1)
        
        return {
            "nancy_advantage_rate": nancy_advantage_rate,
            "intelligence_time_premium": intelligence_premium,
            "capability_superiority": "high" if nancy_advantage_rate > 0.7 else "moderate" if nancy_advantage_rate > 0.5 else "low",
            "efficiency_rating": "excellent" if intelligence_premium < 2 else "good" if intelligence_premium < 3 else "poor"
        }
    
    def _identify_unique_value_factors(self) -> List[str]:
        """Identify unique value factors Nancy provides"""
        unique_factors = []
        
        # Spreadsheet intelligence
        baseline_validation = self.results.get("baseline_validation", {})
        if baseline_validation.get("fairness_achieved"):
            unique_factors.append("Structured data intelligence vs enhanced baseline text search")
        else:
            unique_factors.append("Spreadsheet processing capability (baseline cannot compete)")
        
        # Codebase intelligence
        codebase_results = self.results.get("codebase_intelligence", {})
        if codebase_results and "intelligence_analysis" in codebase_results:
            codebase_advantage = codebase_results["intelligence_analysis"].get("overall_intelligence_advantage", 0)
            if codebase_advantage > 0.6:
                unique_factors.append("Advanced codebase analysis with AST parsing and Git integration")
            elif codebase_advantage > 0.3:
                unique_factors.append("Moderate codebase intelligence improvements")
        
        # Change intelligence
        change_results = self.results.get("change_intelligence", {})
        if change_results and "change_intelligence_analysis" in change_results:
            change_detection = change_results["change_intelligence_analysis"]["overall_performance"].get("change_detection_rate", 0)
            if change_detection > 0.5:
                unique_factors.append("Unique change detection and impact analysis (impossible for baseline)")
            else:
                unique_factors.append("Change intelligence capability (baseline cannot provide)")
        
        # Four-brain architecture
        unique_factors.append("Multi-brain orchestration for complex engineering queries")
        unique_factors.append("Cross-data-type relationship discovery and analysis")
        
        return unique_factors
    
    def _perform_cost_benefit_analysis(self) -> Dict[str, Any]:
        """Perform cost-benefit analysis"""
        benchmark_results = self.results.get("comprehensive_benchmark", {})
        
        if not benchmark_results or "comprehensive_analysis" not in benchmark_results:
            return {"status": "insufficient_data"}
        
        cost_analysis = benchmark_results["comprehensive_analysis"].get("intelligence_cost_analysis", {})
        capabilities_gained = cost_analysis.get("capabilities_gained", 0)
        time_premium = cost_analysis.get("intelligence_time_premium", 1)
        
        # Estimate engineering time savings
        if capabilities_gained > 10:
            estimated_time_savings = "High - Nancy handles complex queries that would require manual research"
        elif capabilities_gained > 5:
            estimated_time_savings = "Moderate - Nancy reduces time for multi-step analysis"  
        else:
            estimated_time_savings = "Low - Limited capability advantages shown"
        
        return {
            "intelligence_time_premium": time_premium,
            "capabilities_gained": capabilities_gained,
            "estimated_engineering_time_savings": estimated_time_savings,
            "cost_per_capability": cost_analysis.get("cost_per_capability", 0),
            "roi_assessment": "positive" if time_premium < 3 and capabilities_gained > 5 else "mixed" if capabilities_gained > 3 else "negative"
        }
    
    def _calculate_engineering_roi(self) -> str:
        """Calculate ROI for engineering teams"""
        capability_comparison = self._assess_capability_comparison()
        cost_benefit = self._perform_cost_benefit_analysis()
        unique_factors = self._identify_unique_value_factors()
        
        # ROI factors
        advantage_rate = capability_comparison.get("nancy_advantage_rate", 0)
        time_premium = capability_comparison.get("intelligence_time_premium", 1)
        unique_capability_count = len(unique_factors)
        
        if advantage_rate > 0.7 and time_premium < 2.5 and unique_capability_count >= 4:
            return "High ROI - Nancy provides significant engineering value with reasonable cost"
        elif advantage_rate > 0.5 and time_premium < 3.5 and unique_capability_count >= 3:
            return "Positive ROI - Nancy offers meaningful improvements for engineering workflows"
        elif advantage_rate > 0.3 and unique_capability_count >= 2:
            return "Mixed ROI - Some value demonstrated, evaluate specific engineering use cases"
        else:
            return "Questionable ROI - Limited advantages relative to costs"
    
    def _assess_deployment_readiness(self) -> Dict[str, Any]:
        """Assess deployment readiness"""
        reliability = self._assess_system_reliability()
        capability = self._assess_capability_comparison()
        
        nancy_stable = reliability["nancy_reliability"] in ["high", "moderate"]
        good_performance = capability["nancy_advantage_rate"] > 0.5
        reasonable_cost = capability["intelligence_time_premium"] < 4
        
        if nancy_stable and good_performance and reasonable_cost:
            readiness = "ready"
            recommendation = "Deploy to engineering teams with proper training"
        elif nancy_stable and good_performance:
            readiness = "pilot_ready"
            recommendation = "Run pilot deployment with selected engineering workflows"
        elif nancy_stable:
            readiness = "development_needed"
            recommendation = "Continue development - system is stable but needs performance improvement"
        else:
            readiness = "not_ready"
            recommendation = "Address stability issues before deployment"
        
        return {
            "readiness_level": readiness,
            "deployment_recommendation": recommendation,
            "readiness_factors": {
                "system_stability": nancy_stable,
                "performance_advantage": good_performance,
                "cost_effectiveness": reasonable_cost
            }
        }
    
    def _assess_competitive_advantage(self) -> str:
        """Assess competitive advantage over baseline RAG"""
        unique_factors = self._identify_unique_value_factors()
        capability_comparison = self._assess_capability_comparison()
        
        advantage_rate = capability_comparison.get("nancy_advantage_rate", 0)
        unique_capability_count = len(unique_factors)
        
        if advantage_rate > 0.8 and unique_capability_count >= 5:
            return "Strong competitive advantage - Nancy provides capabilities baseline cannot match"
        elif advantage_rate > 0.6 and unique_capability_count >= 4:
            return "Moderate competitive advantage - Clear differentiation from baseline RAG"
        elif advantage_rate > 0.4 and unique_capability_count >= 3:
            return "Limited competitive advantage - Some unique capabilities demonstrated"
        else:
            return "Minimal competitive advantage - Consider further development"
    
    def _generate_final_recommendation(self, executive_metrics: Dict[str, Any]) -> str:
        """Generate final deployment recommendation"""
        deployment = executive_metrics["deployment_readiness"]
        roi = executive_metrics["engineering_team_roi"]
        competitive = executive_metrics["competitive_advantage"]
        
        if deployment["readiness_level"] == "ready" and "High ROI" in roi:
            return "STRONGLY RECOMMEND: Deploy Nancy to engineering teams. Clear value proposition with manageable costs."
        elif deployment["readiness_level"] in ["ready", "pilot_ready"] and "Positive ROI" in roi:
            return "RECOMMEND: Deploy Nancy with phased rollout. Monitor usage and optimize based on feedback."
        elif deployment["readiness_level"] == "pilot_ready" and "Mixed ROI" in roi:
            return "PILOT: Run targeted pilot with high-value engineering use cases. Evaluate before full deployment."
        elif deployment["readiness_level"] == "development_needed":
            return "CONTINUE DEVELOPMENT: System shows promise but needs improvement before deployment."
        else:
            return "DEFER DEPLOYMENT: Address critical issues before reconsidering deployment."
    
    def _extract_key_decision_factors(self, executive_metrics: Dict[str, Any]) -> List[str]:
        """Extract key factors for decision making"""
        factors = []
        
        # System reliability
        reliability = executive_metrics["system_reliability"]
        if reliability["nancy_reliability"] == "high":
            factors.append("‚úì Nancy four-brain architecture is stable and reliable")
        else:
            factors.append("‚ö† Nancy system reliability needs attention")
        
        # Capability advantage
        capability = executive_metrics["capability_comparison"]
        if capability["nancy_advantage_rate"] > 0.6:
            factors.append(f"‚úì Nancy demonstrates {capability['nancy_advantage_rate']:.0%} capability advantage over baseline")
        else:
            factors.append(f"‚ö† Nancy shows {capability['nancy_advantage_rate']:.0%} capability advantage - may be insufficient")
        
        # Unique value
        unique_factors = executive_metrics["unique_value_factors"]
        factors.append(f"‚úì Nancy provides {len(unique_factors)} unique capabilities baseline cannot match")
        
        # Cost consideration
        cost_benefit = executive_metrics["cost_benefit_analysis"]
        if cost_benefit.get("roi_assessment") == "positive":
            factors.append("‚úì Positive cost-benefit ratio for engineering teams")
        elif cost_benefit.get("roi_assessment") == "mixed":
            factors.append("‚ö† Mixed cost-benefit - evaluate specific use cases")
        else:
            factors.append("‚úó Questionable cost-benefit ratio")
        
        return factors
    
    def _suggest_implementation_timeline(self, recommendation: str) -> Dict[str, str]:
        """Suggest implementation timeline based on recommendation"""
        if "STRONGLY RECOMMEND" in recommendation:
            return {
                "phase_1": "Immediate: Deploy to 2-3 engineering teams (Month 1)",
                "phase_2": "Expand to all engineering teams with training (Month 2-3)",
                "phase_3": "Full deployment and optimization (Month 4-6)"
            }
        elif "RECOMMEND" in recommendation:
            return {
                "phase_1": "Pilot with 1-2 engineering teams (Month 1-2)",
                "phase_2": "Evaluate results and refine deployment (Month 3)",
                "phase_3": "Gradual rollout to all teams (Month 4-8)"
            }
        elif "PILOT" in recommendation:
            return {
                "phase_1": "Select high-value use cases for pilot (Month 1)",
                "phase_2": "Run 3-month pilot with metrics collection (Month 2-4)",
                "phase_3": "Evaluate pilot results and decide on deployment (Month 5-6)"
            }
        else:
            return {
                "phase_1": "Continue development and address critical issues (Month 1-3)",
                "phase_2": "Re-evaluate system capabilities (Month 4)",
                "phase_3": "Consider pilot if improvements are significant (Month 5-6)"
            }
    
    def _perform_risk_assessment(self, executive_metrics: Dict[str, Any]) -> Dict[str, List[str]]:
        """Perform risk assessment"""
        risks = {
            "high": [],
            "medium": [],
            "low": []
        }
        
        # System reliability risks
        reliability = executive_metrics["system_reliability"]
        if reliability["nancy_reliability"] == "low":
            risks["high"].append("Nancy system instability could disrupt engineering workflows")
        elif reliability["nancy_reliability"] == "moderate":
            risks["medium"].append("Nancy system may have occasional reliability issues")
        
        # Performance risks
        capability = executive_metrics["capability_comparison"]
        if capability["intelligence_time_premium"] > 4:
            risks["high"].append("High query latency may frustrate engineering users")
        elif capability["intelligence_time_premium"] > 2.5:
            risks["medium"].append("Moderate query latency - users may need adaptation time")
        
        # ROI risks
        cost_benefit = executive_metrics["cost_benefit_analysis"]
        if cost_benefit.get("roi_assessment") == "negative":
            risks["high"].append("Negative ROI could lead to deployment failure")
        elif cost_benefit.get("roi_assessment") == "mixed":
            risks["medium"].append("Mixed ROI - deployment success depends on use case selection")
        
        # Adoption risks
        unique_factors = executive_metrics["unique_value_factors"]
        if len(unique_factors) < 3:
            risks["medium"].append("Limited unique value may reduce user adoption")
        else:
            risks["low"].append("Strong unique value proposition supports adoption")
        
        return risks
    
    def _print_executive_summary(self, executive_analysis: Dict[str, Any]):
        """Print executive summary to console"""
        print("\n" + "="*80)
        print("üìä EXECUTIVE SUMMARY - NANCY VALIDATION RESULTS")
        print("="*80)
        
        metrics = executive_analysis["executive_metrics"]
        
        print(f"\nüéØ FINAL RECOMMENDATION:")
        print(f"   {executive_analysis['final_recommendation']}")
        
        print(f"\nüíº ENGINEERING TEAM ROI:")
        print(f"   {metrics['engineering_team_roi']}")
        
        print(f"\nüèÜ COMPETITIVE ADVANTAGE:")
        print(f"   {metrics['competitive_advantage']}")
        
        print(f"\nüîë KEY DECISION FACTORS:")
        for factor in executive_analysis['key_decision_factors']:
            print(f"   {factor}")
        
        print(f"\n‚ö†Ô∏è  RISK ASSESSMENT:")
        risks = executive_analysis['risk_assessment']
        if risks['high']:
            print("   HIGH RISKS:")
            for risk in risks['high']:
                print(f"     ‚Ä¢ {risk}")
        if risks['medium']:
            print("   MEDIUM RISKS:")
            for risk in risks['medium']:
                print(f"     ‚Ä¢ {risk}")
        
        print(f"\nüìÖ IMPLEMENTATION TIMELINE:")
        timeline = executive_analysis['implementation_timeline']
        for phase, description in timeline.items():
            print(f"   {phase.upper()}: {description}")
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run the complete validation suite"""
        print("üöÄ COMPREHENSIVE NANCY VALIDATION SUITE")
        print("   Enhanced Baseline vs Nancy Four-Brain Architecture")
        print("   Complete Capability Analysis and ROI Assessment")
        print("="*80)
        
        total_start = time.time()
        
        # Phase 1: System Health Check (Critical)
        print(f"\n{datetime.now().strftime('%H:%M:%S')} - Phase 1/6: System Health Check")
        self.results["system_health"] = self.check_system_health()
        
        if self.results["system_health"]["overall_health"] == "poor":
            print("‚ùå Critical systems are unhealthy - cannot proceed with validation")
            return {"status": "aborted", "reason": "System health check failed"}
        
        # Phase 2: Enhanced Baseline Validation (Critical)
        print(f"\n{datetime.now().strftime('%H:%M:%S')} - Phase 2/6: Enhanced Baseline Validation")
        self.results["baseline_validation"] = self.run_enhanced_baseline_validation()
        
        # Phase 3: Comprehensive Benchmark (Critical)
        print(f"\n{datetime.now().strftime('%H:%M:%S')} - Phase 3/6: Comprehensive Capability Benchmark")
        self.results["comprehensive_benchmark"] = self.run_comprehensive_benchmark()
        
        # Phase 4: Codebase Intelligence (Optional)
        print(f"\n{datetime.now().strftime('%H:%M:%S')} - Phase 4/6: Codebase Intelligence Testing")
        try:
            self.results["codebase_intelligence"] = self.run_codebase_intelligence_test()
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Codebase intelligence test skipped: {e}")
            self.results["codebase_intelligence"] = {"status": "skipped", "error": str(e)}
        
        # Phase 5: Change Intelligence (Optional) 
        print(f"\n{datetime.now().strftime('%H:%M:%S')} - Phase 5/6: Change Intelligence Testing")
        try:
            self.results["change_intelligence"] = self.run_change_intelligence_test()
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Change intelligence test skipped: {e}")
            self.results["change_intelligence"] = {"status": "skipped", "error": str(e)}
        
        # Phase 6: Executive Analysis (Critical)
        print(f"\n{datetime.now().strftime('%H:%M:%S')} - Phase 6/6: Executive Analysis")
        self.results["executive_analysis"] = self.generate_executive_analysis()
        
        total_time = time.time() - total_start
        
        # Compile final results
        final_results = {
            "metadata": {
                "validation_suite_version": "1.0",
                "timestamp": self.test_start_time.isoformat(),
                "total_validation_time": total_time,
                "nancy_url": self.nancy_url,
                "baseline_url": self.baseline_url,
                "phases_completed": len([r for r in self.results.values() if r.get("status") != "skipped"])
            },
            "validation_results": self.results,
            "executive_summary": self.results["executive_analysis"]
        }
        
        # Save comprehensive results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"nancy_comprehensive_validation_{timestamp}.json"
        
        with open(filename, 'w') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        print(f"\nüìÅ COMPLETE VALIDATION RESULTS SAVED: {filename}")
        print(f"‚è±Ô∏è  Total validation time: {total_time/60:.1f} minutes")
        
        return final_results


def main():
    """Run the comprehensive validation suite"""
    
    print("Nancy Comprehensive Validation Suite")
    print("Validating four-brain architecture against enhanced baseline RAG")
    print("This test represents the culmination of Nancy's major enhancements\n")
    
    validation_suite = ComprehensiveValidationSuite()
    
    try:
        results = validation_suite.run_comprehensive_validation()
        
        if results and "executive_summary" in results:
            print("\nüéâ VALIDATION SUITE COMPLETED SUCCESSFULLY")
            print("Executive summary has been generated with deployment recommendations")
            return results
        else:
            print("\n‚ùå VALIDATION SUITE FAILED")
            print("Check system health and try again")
            return None
            
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Validation suite interrupted by user")
        print("Partial results may be available")
        return None
    except Exception as e:
        print(f"\n\n‚ùå Validation suite failed with error: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()